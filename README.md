# awesome-knowledge-distillation [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Cited by [Awesome Knowledge distillation](https://github.com/dkozlov/awesome-knowledge-distillation)

## History

- [2019 years](#2019)
- [2018 years](#2018)
- [2017 years](#2017)
- [2016 years](#2016)
- [2015 years](#2015)

### Goal

|  Category |  `C` |  `P`  |  `Other` |
|:------------|:--------------:|:----------------------:|:----------:|
| Explanation | Model compression | Performance up | Other types |

### 2019
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |
| [Relational Knowledge Distillation](https://arxiv.org/abs/1904.05068) | CVPR | `P` | [GitHub](https://github.com/lenscloth/RKD) |

### 2018
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |
| [Born-Again Neural Networks](https://arxiv.org/abs/1805.04770) | ICML | `P` | - |
| [Label Refinery: Improving ImageNet Classification through Label Progression](https://arxiv.org/abs/1805.02641) | - | `P` | [GitHub](https://github.com/hessamb/label-refinery) |

### 2017
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |
| [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://arxiv.org/abs/1612.03928) | ICLR | `C` | [GitHub](https://github.com/szagoruyko/attention-transfer) |

### 2016
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |

### 2015
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |
| [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550) | ICLR | `C` | [GitHub](https://github.com/adri-romsor/FitNets/tree/master/costs) |
| [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) | NIPS | `C` | - |
