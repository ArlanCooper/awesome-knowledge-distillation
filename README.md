# awesome-knowledge-distillation
Inspired by [Awesome Knowledge distillation](https://github.com/dkozlov/awesome-knowledge-distillation)

## History

- [2019 years](#2019)
- [2018 years](#2018)
- [2017 years](#2017)
- [2016 years](#2016)
- [2015 years](#2015)

### 2019
|   Title  | Issue | Release |
| :--------| :---: | :-----: |

### 2018
|   Title  | Issue | Release |
| :--------| :---: | :-----: |

### 2017
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://arxiv.org/abs/1612.03928) | ICLR | [GitHub](https://github.com/szagoruyko/attention-transfer) |

### 2016
|   Title  | Issue | Release |
| :--------| :---: | :-----: |

### 2015
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550) | ICLR | [GitHub](https://github.com/adri-romsor/FitNets/tree/master/costs) |
| [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) | NIPS | - |
