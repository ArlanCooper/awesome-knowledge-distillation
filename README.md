# awesome-knowledge-distillation [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Cited by [Awesome Knowledge distillation](https://github.com/dkozlov/awesome-knowledge-distillation)

## History

- [2019 years](#2019)
- [2018 years](#2018)
- [2017 years](#2017)
- [2016 years](#2016)
- [2015 years](#2015)

### Goal

|  Category |  `C` |  `P`  |  `Other` |
|:------------|:--------------:|:----------------------:|:----------:|
| Explanation | Model compression | Performance up | Other types |

### 2019
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |
| [Relational Knowledge Distillation](https://arxiv.org/abs/1904.05068) | CVPR | `P` | [GitHub](https://github.com/lenscloth/RKD) |
| [A Comprehensive Overhaul of Feature Distillation](https://arxiv.org/abs/1904.01866) | ICCV | `C` | [GitHub](https://github.com/clovaai/overhaul-distillation) |
| [Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons](https://arxiv.org/abs/1811.03233) | AAAI | `Other` | [GitHub](https://github.com/bhheo/AB_distillation) |
| [Similarity-Preserving Knowledge Distillation](https://arxiv.org/abs/1907.09682) | ICCV | `C` | - |
| [Knowledge Distillation via Instance Relationship Graph]() | CVPR | `P` | [GitHub](https://github.com/yufanLIU/IRG) |

### 2018
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |
| [Born-Again Neural Networks](https://arxiv.org/abs/1805.04770) | ICML | `P` | - |
| [Label Refinery: Improving ImageNet Classification through Label Progression](https://arxiv.org/abs/1805.02641) | - | `P` | [GitHub](https://github.com/hessamb/label-refinery) |
| [Knowledge Transfer with Jacobian Matching](https://arxiv.org/abs/1803.00443) | ICML |  | - |
| [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer) | NIPS | `C` | - |



### 2017
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |
| [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://arxiv.org/abs/1612.03928) | ICLR | `C` | [GitHub](https://github.com/szagoruyko/attention-transfer) |
| [A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf) | CVPR | `C` | - |

### 2016
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |


### 2015
|   Title  | Issue | Category | Release |
| :--------| :---: | :-----:  | :-----: |
| [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550) | ICLR | `C` | [GitHub](https://github.com/adri-romsor/FitNets) |
| [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) | NIPS | `C` | - |
