# awesome-knowledge-distillation [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Hello visitors, I have been interested in effcient deep neural networks design, such as pruning, autoML, quantization and also focused on the knowledge distillation for successful network generalization. This page organizes for the knowledge distillation.

## History

- [2020 years](#2020)
- [2019 years](#2019)
- [2018 years](#2018)
- [2017 years](#2017)
- [2016 years](#2016)
- [2015 years](#2015)


### 2020
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space](https://papers.nips.cc/paper/2020/file/91c77393975889bd08f301c9e13a44b7-Paper.pdf) | NeurIPS | [GitHub](https://github.com/AnTuo1998/AE-KD.) |
| [Revisiting Knowledge Distillation via Label Smoothing Regularization](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf) | CVPR | [GitHub](https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation) |
| [Contrastive Representation Distillation](https://arxiv.org/abs/1910.10699) | ICLR | [Github](https://github.com/HobbitLong/RepDistiller) |
| [Online Knowledge Distillation with Diverse Peers](https://aaai.org/Papers/AAAI/2020GB/AAAI-ChenD.4552.pdf) | AAAI | [Github](https://github.com/DefangChen/OKDDip-AAAI2020) |
| [Ensemble Distribution Distillation](https://openreview.net/pdf?id=BygSP6Vtvr) | ICLR | - |


### 2019
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Relational Knowledge Distillation](https://arxiv.org/abs/1904.05068) | CVPR | [Github](https://github.com/lenscloth/RKD) |
| [A Comprehensive Overhaul of Feature Distillation](https://arxiv.org/abs/1904.01866) | ICCV | [Github](https://github.com/clovaai/overhaul-distillation) |
| [Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons](https://arxiv.org/abs/1811.03233) | AAAI | [Github](https://github.com/bhheo/AB_distillation) |
| [Similarity-Preserving Knowledge Distillation](https://arxiv.org/abs/1907.09682) | ICCV | - |
| [Knowledge Distillation via Instance Relationship Graph](http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.pdf) | CVPR | [Github](https://github.com/yufanLIU/IRG) |
| [Diversity with Cooperation: Ensemble Methods for Few-Shot Classification](http://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf) | ICCV | [Github](https://github.com/dvornikita/fewshot_ensemble) |

### 2018
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Born-Again Neural Networks](https://arxiv.org/abs/1805.04770) | ICML | - |
| [Label Refinery: Improving ImageNet Classification through Label Progression](https://arxiv.org/abs/1805.02641) | - | [Github](https://github.com/hessamb/label-refinery) |
| [Knowledge Transfer with Jacobian Matching](https://arxiv.org/abs/1803.00443) | ICML | - |
| [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer) | NeurIPS | - |
| [Knowledge Distillation by On-the-Fly Native Ensemble](https://papers.nips.cc/paper/7980-knowledge-distillation-by-on-the-fly-native-ensemble.pdf) | NeurIPS | [Github](https://github.com/Lan1991Xu/ONE_NeurIPS2018) |
| [Collaborative Learning for Deep Neural Networks](https://papers.nips.cc/paper/7454-collaborative-learning-for-deep-neural-networks.pdf) | NeurIPS | - |
| [Deep Mutual Learning](https://zpascal.net/cvpr2018/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf) | CVPR | [Github](https://github.com/chxy95/Deep-Mutual-Learning) |
| [Large Scale Distributed Neural Network Training Through Online Distillation](https://openreview.net/pdf?id=rkr1UDeC-) | ICLR | - |



### 2017
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://arxiv.org/abs/1612.03928) | ICLR | [Github](https://github.com/szagoruyko/attention-transfer) |
| [A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf) | CVPR | - |

### 2015
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550) | ICLR | [Github](https://github.com/adri-romsor/FitNets) |
| [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) | NeurIPS | - |
