# awesome-knowledge-distillation [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Hello visitors, I'm interested in deep neural network reduction, such as pruning, architectural search, tensor approximation, and also focus on leveraging network performance with knowledge distillation. This page puts in order to summarize extension of my work related to knowledge distillation, and comprises mainly learning improvements except for an application-oriented.

## History

- [2020 years](#2020)
- [2019 years](#2019)
- [2018 years](#2018)
- [2017 years](#2017)
- [2016 years](#2016)
- [2015 years](#2015)


### 2020
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Contrastive Representation Distillation](https://arxiv.org/abs/1910.10699) | ICLR | [Github](https://github.com/HobbitLong/RepDistiller) |
| [Online Knowledge Distillation with Diverse Peers](https://aaai.org/Papers/AAAI/2020GB/AAAI-ChenD.4552.pdf) | AAAI | [Github](https://github.com/DefangChen/OKDDip-AAAI2020) |


### 2019
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Relational Knowledge Distillation](https://arxiv.org/abs/1904.05068) | CVPR | [Github](https://github.com/lenscloth/RKD) |
| [A Comprehensive Overhaul of Feature Distillation](https://arxiv.org/abs/1904.01866) | ICCV | [Github](https://github.com/clovaai/overhaul-distillation) |
| [Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons](https://arxiv.org/abs/1811.03233) | AAAI | [Github](https://github.com/bhheo/AB_distillation) |
| [Similarity-Preserving Knowledge Distillation](https://arxiv.org/abs/1907.09682) | ICCV | - |
| [Knowledge Distillation via Instance Relationship Graph](http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.pdf) | CVPR | [Github](https://github.com/yufanLIU/IRG) |

### 2018
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Born-Again Neural Networks](https://arxiv.org/abs/1805.04770) | ICML | - |
| [Label Refinery: Improving ImageNet Classification through Label Progression](https://arxiv.org/abs/1805.02641) | - | [Github](https://github.com/hessamb/label-refinery) |
| [Knowledge Transfer with Jacobian Matching](https://arxiv.org/abs/1803.00443) | ICML | - |
| [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer) | NIPS | - |
| [Knowledge Distillation by On-the-Fly Native Ensemble](https://papers.nips.cc/paper/7980-knowledge-distillation-by-on-the-fly-native-ensemble.pdf) | NIPS | [Github](https://github.com/Lan1991Xu/ONE_NeurIPS2018) |



### 2017
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://arxiv.org/abs/1612.03928) | ICLR | [Github](https://github.com/szagoruyko/attention-transfer) |
| [A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf) | CVPR | - |

### 2015
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550) | ICLR | [Github](https://github.com/adri-romsor/FitNets) |
| [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) | NIPS | - |
